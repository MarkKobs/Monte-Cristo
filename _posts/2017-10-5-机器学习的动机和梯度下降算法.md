---
title: 机器学习的动机和梯度下降算法
date: 2017-10-05 00:23:23
tags: AndrewNg
category: MachineLearning
---

# 机器学习的动机和应用
回归和分类有一个很大的区别就是标准答案一个是连续的，一个是离散的。有一个变量或者两个变量的时候可以在二维空间画出图像进行分类，但是假如变量有无限个，无限维空间的图像怎么画呢？这时候就有一种技术叫做支持向量机的东西！他可以把无限维向量映射到计算机内存中。
### unsupervised learning
当我们没有一个标准答案的时候，这样的学习就称为是无监督学习。百度地图上3D视图的创建的第一步就是用无监督学习的聚类算法进行分析的。
### reimforcement learning
加强学习为了写出由人写不出来的算法，它的关键在于它的reward funciton（回报函数），这种机制类似于在训练自己的小狗，假如小狗做了对的事情，你就给它食物(good dog),假如它做了错事，你就惩罚他(bad dog),当然在实际的学习中不用像这样惩罚，但是机制是一样的。这个理论可以广泛应用在机器人身上和游戏领域。
# 监督学习应用梯度下降
## 学习大纲
- liner regression线性回归
- gradient descent梯度下降
- normal equations正规方程组
在视频当中，吴恩达放了关于一个类似于无人驾驶的东西，我在联想，机器学习还可以应用在犯罪鉴定当中，每个人都有自己的“行动DNA”，我更喜欢把它叫做行为DNA感觉更cool，可以被学习，然后具有自己的特征，这样的技术不是太牛？
### 梯度下降的学习方法
#### 一.linear regression
1. 假设单变量，只有一个x
![image](https://latex.codecogs.com/gif.latex?h_\theta&space;(x)=\theta&space;_0&plus;\theta_1x)
2. 如何判断拟合的好不好，通常我们要使用一个叫做Cost Function的代价函数，它的目标是：给定输入变量x，向量y，theta向量，输出cost的值，而且是越小越好
![image](https://latex.codecogs.com/gif.latex?J(\theta&space;_0,\theta&space;_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta&space;(x^(i))-y^(i))^2)
3. 如果是线性回归，那么cost的函数一定是bowl shape.
4. 将以上两个公式合并，便得到了梯度下降的演化公式。

这里不再详细讲解，提供一个链接，有一些习题可以测试自己是否理解。
http://blog.csdn.net/xiazdong/article/details/7950084